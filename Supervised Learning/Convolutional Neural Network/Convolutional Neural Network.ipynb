{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this we will be tranning a CNN to classify dog or cat,\n",
    "But before doing this we need to do Image-Preprocessing,\n",
    "Here we have images of cats and dogs seperated in folders correctly, with correct name with serialno so\n",
    "One solution is to extract label from the images and use it for supervised learning but this is not efficient approach\n",
    "Keras Library has a solution for it we will see it in a while.\n",
    "\n",
    "In Keras we just need to create simple structure which will help us in supervised learning from images without even \n",
    "accessing their names from the image labels.\n",
    "\n",
    "First of all what we need to do is that we need to make training set and test set with images of the classes in the folder separetly, for ex like in training set folder we have to folder cats and dogs with images in them separated, so we have to do same with the test set, through this structure Keras will understand what is the correct label, you have done this before in the inception model in the transfer learning model\n",
    "\n",
    "It is really important to use Feature Scaling in Deep Learning and CV applications we will use it later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we have do not need to do Data-Preprocessing so we will start from building our CNN model\n",
    "# Part-1 : Building CNN Model\n",
    "\n",
    "# Importing the important packages and libraries\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, MaxPooling2D, Flatten\n",
    "# Here we use Sequential model to initialize our NN we have two ways one is Sequential and other is Graph\n",
    "# In sequential we can add layers to the NN because CNN is also made from layers only\n",
    "# As you can see that in the layer we have Convolution2D because we ae going to work on images and \n",
    "# if we want to apply CNN to videos then we can apply Convolution3D because we have time also being one dimension\n",
    "# other is just the same that are required for building the CNN that is Convolution Layer, Pooling layer and then Flatten\n",
    "# Dense package is used for fully Connected Layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tensorflow'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is just to verify that we have tensorflow Backend enabled because it is faster than theano\n",
    "# We have /home/sparsh/anaconda2/envs/py35/etc/conda/activate.d \n",
    "# In this file we have changed the backend from theano to tensorflow\n",
    "import keras\n",
    "keras.backend.backend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initializing the CNN\n",
    "model = Sequential()\n",
    "\n",
    "# Adding the Convolution Layer\n",
    "model.add(Conv2D(32, (3, 3), input_shape=(64,64,3), activation='relu'))\n",
    "# Here as we have studied in the CNN model we have used 32 feature detectors in the model and 3X3 is the size of the \n",
    "# feature detector and input_shape is the shape of the image we enter in the model for processing if you are using theano\n",
    "# then the order would change see the documentation and here we are using activations as relu\n",
    "# You should see the changes by googling them because the library are getting updated day by day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now Pooling the Feature Maps to reduce the size to reduce complexity without compromising performance\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "# 2X2 is the pool detector in general we take this because loosing information\n",
    "# This step will reduce the size of the feature map and it will computionlly easy to process without lossing info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why we are taking so much pain in creating a convolution layer and pooling layer, why dont we just flatten the input image just input it to the ANN?\n",
    "\n",
    "The answer to above question is that if we input the image without convolution and pooling then we are just simply entering the all the pixels of the image and their is spatial realtion between the feature so classification becomes difficult and computationally expensive\n",
    "\n",
    "But if we are doing the convolution layer and pooling we are extracating certain feature of the image and then we also reducing the size of the image and then we are applying the ANN but here we have also preserved the spatial oreintation of the image and we have also preserved some relationship between the pixel using the feature detectors.\n",
    "\n",
    "SO HIGHLY RECOMMENDED TO USE CNN FOR IMAGE CLASSIFICATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now we will increase the accuracy more by inserting a second convolution layer\n",
    "model.add(Conv2D(32, (3, 3), activation='relu'))\n",
    "# And we have to also repeat  the pooling step inorder to increase accuracy\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Flattening Step\n",
    "model.add(Flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now we have to add Fully Connected Hidden layer in the Network \n",
    "model.add(Dense(units=128, activation='relu'))\n",
    "model.add(Dense(units=1, activation='sigmoid'))\n",
    "# Here output_dim is replaced with units ie number of the nodes in the layer\n",
    "# Here we are using sigmoid function because we have binary classification but if more than 2 class we should use \n",
    "# softmax function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Compile Step\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now we will do Image Augmentation because it is important to prevent overfitting as we donot have a lot of images\n",
    "# so we have to use this trick to train our model and prevent it from overfitting\n",
    "# You can consider this as image processing step\n",
    "\n",
    "# So what this image augmentation trick will do is that it will divide our training set into batches and then inside\n",
    "# these batches the algorithm will rotate shift and trnasform the images and this will give us many more different \n",
    "# variations in the training set and we can train our model on different example this will make our model\n",
    "# more GENRALIZED and it will predict better\n",
    "\n",
    "# SUMMARY Image Augumentation\n",
    "# It is a technique that allows us to enrich our dataset our training set without adding more images\n",
    "# and therefore allows us to get better results with no or litte overfitting even with the small amounts of images\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This library is used for the technique discussed above\n",
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(rescale=1./255, shear_range=0.2, zoom_range=0.2, horizontal_flip=True)\n",
    "# Here we are creating a object of ImageDataGenerator class with these specifications to be applied in the \n",
    "# training set, Similary with test data\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# We know that our pixel value lie between 0-255 by using rescale we will pixel value between 0-1 because 1./255 will be\n",
    "# multiplied to the pixel value, rescaling is very important, it is just changing pixel value not the size of the image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8000 images belonging to 2 classes.\n",
      "Found 2000 images belonging to 2 classes.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(64, 64, 3)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_set = train_datagen.flow_from_directory('dataset/training_set', target_size=(64,64), batch_size=32, \n",
    "                                                 class_mode='binary')\n",
    "\n",
    "test_set = train_datagen.flow_from_directory('dataset/test_set/', target_size=(64,64), batch_size=32, \n",
    "                                                 class_mode='binary')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "  42/8000 [..............................] - ETA: 4731s - loss: 0.7035 - acc: 0.5007"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-15f4bd8fe316>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Here we are using fit generator method to fit our training set because our training data is generated by ImageData\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# Generator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda2/envs/py35/lib/python3.5/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     85\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     86\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 87\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/py35/lib/python3.5/site-packages/keras/models.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, initial_epoch)\u001b[0m\n\u001b[1;32m   1115\u001b[0m                                         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1116\u001b[0m                                         \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1117\u001b[0;31m                                         initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/py35/lib/python3.5/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     85\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     86\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 87\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/py35/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, initial_epoch)\u001b[0m\n\u001b[1;32m   1807\u001b[0m                 \u001b[0mbatch_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1808\u001b[0m                 \u001b[0;32mwhile\u001b[0m \u001b[0msteps_done\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1809\u001b[0;31m                     \u001b[0mgenerator_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_generator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1811\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__len__'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/py35/lib/python3.5/site-packages/keras/utils/data_utils.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    634\u001b[0m                     \u001b[0;32myield\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    635\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 636\u001b[0;31m                 \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Here we are using fit generator method to fit our training set because our training data is generated by ImageData\n",
    "# Generator\n",
    "model.fit_generator(training_set, steps_per_epoch=8000, epochs=1, validation_data=test_set, validation_steps=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now as we have trained our model we are going to predict for a single example\n",
    "model.save('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[  54.   58.    7.]\n",
      "  [  58.   63.    9.]\n",
      "  [  64.   67.   10.]\n",
      "  ..., \n",
      "  [ 136.  144.   71.]\n",
      "  [ 140.  150.   77.]\n",
      "  [ 139.  149.   78.]]\n",
      "\n",
      " [[  48.   54.    6.]\n",
      "  [  51.   58.    7.]\n",
      "  [  58.   63.    9.]\n",
      "  ..., \n",
      "  [ 129.  137.   64.]\n",
      "  [ 139.  149.   78.]\n",
      "  [ 141.  151.   80.]]\n",
      "\n",
      " [[  48.   56.    7.]\n",
      "  [  48.   56.    7.]\n",
      "  [  54.   61.   10.]\n",
      "  ..., \n",
      "  [ 123.  130.   63.]\n",
      "  [ 136.  145.   80.]\n",
      "  [ 140.  149.   82.]]\n",
      "\n",
      " ..., \n",
      " [[  46.   55.   12.]\n",
      "  [  42.   50.    9.]\n",
      "  [  38.   49.    9.]\n",
      "  ..., \n",
      "  [ 239.  205.  170.]\n",
      "  [ 235.  209.  186.]\n",
      "  [ 229.  202.  173.]]\n",
      "\n",
      " [[  50.   57.   15.]\n",
      "  [  42.   50.    9.]\n",
      "  [  44.   52.   11.]\n",
      "  ..., \n",
      "  [ 234.  200.  162.]\n",
      "  [ 236.  206.  178.]\n",
      "  [ 234.  203.  174.]]\n",
      "\n",
      " [[  53.   59.   13.]\n",
      "  [  43.   51.   10.]\n",
      "  [  49.   56.   12.]\n",
      "  ..., \n",
      "  [ 231.  195.  159.]\n",
      "  [ 235.  213.  190.]\n",
      "  [ 233.  206.  179.]]]\n",
      "[[[[  54.   58.    7.]\n",
      "   [  58.   63.    9.]\n",
      "   [  64.   67.   10.]\n",
      "   ..., \n",
      "   [ 136.  144.   71.]\n",
      "   [ 140.  150.   77.]\n",
      "   [ 139.  149.   78.]]\n",
      "\n",
      "  [[  48.   54.    6.]\n",
      "   [  51.   58.    7.]\n",
      "   [  58.   63.    9.]\n",
      "   ..., \n",
      "   [ 129.  137.   64.]\n",
      "   [ 139.  149.   78.]\n",
      "   [ 141.  151.   80.]]\n",
      "\n",
      "  [[  48.   56.    7.]\n",
      "   [  48.   56.    7.]\n",
      "   [  54.   61.   10.]\n",
      "   ..., \n",
      "   [ 123.  130.   63.]\n",
      "   [ 136.  145.   80.]\n",
      "   [ 140.  149.   82.]]\n",
      "\n",
      "  ..., \n",
      "  [[  46.   55.   12.]\n",
      "   [  42.   50.    9.]\n",
      "   [  38.   49.    9.]\n",
      "   ..., \n",
      "   [ 239.  205.  170.]\n",
      "   [ 235.  209.  186.]\n",
      "   [ 229.  202.  173.]]\n",
      "\n",
      "  [[  50.   57.   15.]\n",
      "   [  42.   50.    9.]\n",
      "   [  44.   52.   11.]\n",
      "   ..., \n",
      "   [ 234.  200.  162.]\n",
      "   [ 236.  206.  178.]\n",
      "   [ 234.  203.  174.]]\n",
      "\n",
      "  [[  53.   59.   13.]\n",
      "   [  43.   51.   10.]\n",
      "   [  49.   56.   12.]\n",
      "   ..., \n",
      "   [ 231.  195.  159.]\n",
      "   [ 235.  213.  190.]\n",
      "   [ 233.  206.  179.]]]]\n"
     ]
    }
   ],
   "source": [
    "# We are using numpy and and image module from the preprocessing module of keras\n",
    "# The image module is used to load the image which we are going to use for making the single prediction\n",
    "# What we do is that we use image.img_to_array function to convert our test image into array of pixels\n",
    "# Our test Image has dimensions like this 64 X 64 X 3\n",
    "import numpy as np\n",
    "from keras.preprocessing import image\n",
    "test_image = image.load_img('dataset/single_prediction/cat_or_dog_1.jpg', target_size = (64, 64))\n",
    "test_image = image.img_to_array(test_image)\n",
    "\n",
    "# np.expand_dims will convert this image of 3 channels into one with axis onto axis=0 or row axis true\n",
    "# And then we will feed this image into our model to predcit the test image we will use the predict method of our model\n",
    "# to predict the category this can only be accesed when you have trained the model\n",
    "# Now what this np.expand_dims will do is that it will add other dimension to the test_image\n",
    "# that is now test_image is like 1 X 64 x 64 X 3\n",
    "# this is because the model.predict method expect this kind of argument to predict the class\n",
    "# so we need to add this dimensions\n",
    "test_image = np.expand_dims(test_image, axis = 0)\n",
    "result = model.predict(test_image)\n",
    "\n",
    "# Now the training_set.class_indices method is use to the know the indices of all the classes\n",
    "training_set.class_indices\n",
    "if result[0][0] == 1:\n",
    "    prediction = 'dog'\n",
    "else:\n",
    "    prediction = 'cat'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dog\n"
     ]
    }
   ],
   "source": [
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
